# Semantic Search Solution for a Gatsby Portfolio

## Overview

Prospective clients often search a portfolio with natural language queries (e.g. *“I want a UI for an AI platform for therapists”*). To meet this need, we propose an **NLP-powered semantic search** system that understands user intent beyond exact keywords. This system will index \~73 portfolio items (case studies in MDX and JSON) and return the most relevant projects for a user’s query. It will use **vector embeddings** to capture semantic meaning and allow searching across multiple facets (project type, problem solved, industry). The solution must be highly accurate, integrate with Gatsby’s React/Node stack, and remain cost-effective (target \~\$20 per 1000 users).

## Requirements for the Search Feature

* **Semantic Understanding**: Interpret the intent and context of user queries, not just exact keyword matches. For example, a query about *“UI for an AI platform for therapists”* should match a project about a mental health AI app, even if the description uses words like *“counselor”* or *“health platform”*.
* **Metadata Awareness**: Extract and utilize key metadata from project descriptions – e.g. **project type** (UI design, UX design, illustration, etc.), **industry/domain** (healthcare, finance, etc.), and **problem solved** (the core challenge or value proposition). This allows filtering or boosting results by these facets.
* **Multi-Aspect Search**: Handle queries that reference multiple categories simultaneously (e.g. a query might specify a desired *design type*, *industry*, and *problem*). The search should be able to match projects on all relevant aspects, using semantic similarity and/or metadata filters.
* **Dynamic Search Results Page**: Provide a results page in Gatsby that displays relevant project cards or links in order of semantic relevance. Results should update dynamically based on the user’s query.
* **Cost Efficiency**: The solution should operate within \~\$20 per 1000 searches/users. This means leveraging affordable or free-tier NLP APIs or open-source tools, and minimal infrastructure overhead. Ideally, each query should cost only a few cents (or less) in API calls or compute.
* **Compatibility with Gatsby**: The tools and architecture must integrate with a Gatsby (React/Node) environment. This implies using Node.js-compatible libraries or REST APIs. We may use serverless functions or a lightweight backend to handle search requests if needed (since Gatsby is a static site generator). The solution should be deployable on platforms like Netlify, Vercel, or a self-hosted Node server.

## Semantic Search Approach (NLP-Powered Search)

**Semantic search** will enable the portfolio search to go beyond keyword matching. It works by encoding text (project descriptions and queries) into high-dimensional **vector embeddings** that capture meaning. Similar content will have embeddings that are close in vector space, allowing us to retrieve results by similarity rather than exact words. In practice:

* We will use a **pre-trained language model** to generate embeddings for each project’s content (and possibly its metadata text). Each embedding is a numeric vector (e.g. 384 to 1536 dimensions) representing the semantic concepts in that project. We index these vectors in a vector store or database.
* At query time, the user’s search phrase is also converted into an embedding using the same model. The system finds the **nearest neighbor vectors** among the project embeddings – i.e. those with highest cosine similarity – which correspond to the most semantically relevant projects.
* This approach can retrieve relevant results even if there are no keyword overlaps. For example, a query containing “therapists” might match a project description containing “counselors” or “mental health professionals” because the embeddings capture their related meaning.

Compared to traditional keyword search, this semantic method better handles synonyms and context. It will improve recall of relevant portfolio items even when phrasing differs. (We can still incorporate a fallback keyword search or hybrid approach for exact matches, but embeddings will be the core.)

## Tools and Options Considered

There are several ways to implement semantic search. We considered **embeddings APIs**, **vector databases**, and **search engines** that could fit our requirements:

### 1. **Embeddings API + Vector Database** (Cloud-Based)

One straightforward approach is to use a hosted NLP API to generate embeddings (such as OpenAI or Cohere), and store them in a vector database for similarity search. This decouples the language understanding (handled by powerful proprietary models) from data storage/retrieval (handled by a specialized vector index):

* **OpenAI Embeddings API** – OpenAI offers state-of-the-art text embedding models (e.g. `text-embedding-ada-002` and newer `text-embedding-3` models). These models have very high accuracy in capturing semantic similarity. Using OpenAI is very simple: you send text and get back a 1536-dimension embedding vector. Cost is **extremely low** (around \$0.0001 per 1K tokens for the Ada-002 model, and even **5× cheaper** for the new embedding-3-small model). This means a short query or project (\~100 tokens) costs on the order of \$0.00001–0.00004 – effectively fractions of a cent. Even 1000 queries would cost well under \$1 in embedding fees. OpenAI provides a Node.js SDK and REST API, making integration straightforward. The downside is data is sent to a third-party (but OpenAI doesn’t train on it by default) and there’s an external dependency, but for our use-case this is acceptable given the accuracy.
* **Cohere Embeddings API** – Cohere provides a similar service for text embeddings. Cohere’s `Embed` model (English) also produces high-quality vectors. Pricing is comparable to OpenAI (on the order of \$0.40 per million tokens embedded, i.e. \$0.0004 per 1K tokens) and they have a Node client. Using Cohere would be as easy as OpenAI – it’s a drop-in alternative if we prefer their model or terms. Both OpenAI and Cohere support **multilingual** content, though our portfolio is presumably English.
* **Vector Database (Managed)** – Once we have embeddings, we need to store and query them efficiently. A managed **vector DB service like Pinecone** is a strong option. Pinecone is a fully managed vector database that offers fast similarity search and metadata filtering out of the box. It has a generous free tier (sufficient for our \~73 vectors and low query volume) and a Node.js client SDK. Pinecone excels in ease of use: we can create an index, upsert our embedding vectors with an ID and metadata, and query by passing a new vector to find similar items. Pinecone also supports adding **metadata filters** alongside vector search (for hybrid queries combining semantic similarity with structured filters) – for example, we could restrict results to `industry:"Healthcare"` if needed. Given our small scale, Pinecone’s free plan would cover it (the standard plan starts at \~\$25/month which is slightly above budget, but likely not needed). Other cloud vector DBs include **Weaviate Cloud** or **Qdrant Cloud**, which also offer free tiers (Qdrant’s free tier allows \~1 million vectors, plenty for us). These are similar in capability – providing fast nearest-neighbor search and filtering via a simple API. They might require a bit more integration work than Pinecone’s polished offering, but are cost-effective (free or very low cost at our scale).

**Evaluation**: Using OpenAI/Cohere with Pinecone (or similar) would give us **top-tier accuracy** (state-of-the-art embeddings) and **ease of implementation**. The cost is minimal: e.g. OpenAI embeddings \~\$0.50 per 1000 queries + Pinecone free tier. Compatibility is excellent – all components have Node/REST interfaces. The main trade-off is reliance on external APIs/services. However, given the small dataset, an alternative is to **forgo a separate DB** and do the vector math ourselves (embedding generation via API, then an in-memory search). With only 73 items, a simple Node function could load all vectors and compute cosine similarity on the fly – this might eliminate the need for Pinecone, yet still use OpenAI for accuracy. This “serverless function + embed API” approach is ultra-simple and cost-effective (no DB hosting cost at all). We’ll compare these options in a table shortly.

### 2. **All-in-One Search Engines** (Self-Hosted or Cloud)

Another route is to use a search engine that has semantic capabilities built in, such as **Typesense** or **Vespa**:

* **Typesense (Vector Search)** – Typesense is an open-source search engine (an alternative to Algolia) known for its easy setup and JSON-based index. Recent versions of Typesense natively support **vector embeddings and semantic search**. You can define an “embedding” field in your index schema and either let Typesense generate embeddings using a built-in model (like a small **MiniLM Sentence-BERT** model) or configure it to call out to an external API (OpenAI, Cohere, etc.) for embeddings. Typesense even supports loading custom ONNX models for embeddings, or using open-source ones via their integration with HuggingFace models. This means with Typesense, we could have a one-stop solution: when we index a document (project) with its text, Typesense will automatically create and store the embedding vector alongside, and at query time it can handle vector similarity searches by itself. It also supports **hybrid search** (combining traditional text ranking with vector similarity) and can apply filters on fields easily (project type, etc., as regular facets). The upside: we manage one service and get both lexical and semantic search. The downside: we have to host Typesense or use Typesense Cloud. Hosting is not too heavy – it’s built in C++ and can run on a modest VM (even a \$5–\$10/mo instance). The resource usage for \~73 docs is negligible, and Typesense is designed to be fast even on minimal hardware. Typesense Cloud is an option if we want a managed service, but pricing for their cloud might not fit \$20/mo (though a small developer tier could). Given our scale, self-hosting Typesense (Docker or binary) is feasible. Integration with Gatsby: Typesense has a well-documented JavaScript client for search queries, so our frontend could query it directly (with a read-only API key) or via a proxy function.
* **Vespa** – Vespa (by Yahoo/Oath) is a powerful open-source search platform that supports **vector search, keyword search, and advanced ranking** at large scale. Vespa could definitely handle our use case; we could push our project data and embeddings into Vespa and use its query API to combine semantic similarity with filters or even custom ranking functions. It’s very flexible (supports deploying custom ML models for reranking, etc.). However, Vespa is likely **overkill** here. It has a steep learning curve and typically runs on a cluster or at least a beefy node. Vespa Cloud exists, but the cost is oriented toward enterprise scales (e.g. it might cost tens or hundreds of dollars a month even for a modest setup). For example, Vespa Cloud pricing for vector search shows \~\$3.36/hour for millions of vectors (far beyond our needs). For 73 items, Vespa is not cost-effective or necessary. We would also need to generate embeddings via an external model (Vespa doesn’t provide a built-in text embedder out of the box; you’d typically preprocess with OpenAI or a transformer and feed vectors in). In short, while Vespa is extremely capable (and could seamlessly do hybrid semantic + textual search), it’s not the most *practical* choice for a small personal portfolio site with a \$20 budget. The engineering effort and cost outweigh the benefits in this scenario.

**Evaluation**: Typesense emerges as a strong all-in-one solution if we prefer self-hosting and want tight integration of semantic search in our stack. Its **accuracy** depends on the embedding model used: the default model (MiniLM) is decent but slightly less accurate than OpenAI’s larger model. However, Typesense can be configured to use OpenAI’s embeddings too, achieving equal accuracy. **Ease of use** is moderate: setting up a Typesense server and populating it requires some initial work, but their API is straightforward and documentation is good. We’d handle updates to the index whenever content changes (which could be done as part of the Gatsby build or via a script). The **cost** can be very low if self-hosted (just the VM cost; and it’s lightweight). Gatsby compatibility is good, since we can either query Typesense directly from the frontend (the case studies are public data, so that’s possible with a safe API key) or through a Node function.

### 3. **Open-Source Embedding + Self-Hosted Vector Store**

A third approach is a hybrid of the above: use an **open-source embedding model** (instead of paying an API) and store vectors in a **self-hosted vector DB or simple index**. For example:

* Use a Python library like **Sentence Transformers (Sentence-BERT)** to generate embeddings for all project texts offline. There are many pre-trained models; one popular lightweight model is `all-MiniLM-L6-v2` (384 dimensions) or `all-MiniLM-L12-v2` (768d), which are fast and fairly accurate. Newer open models (e.g. instructors or MPNet) also achieve good semantic similarity scores. The quality may be slightly below OpenAI’s latest, but still very good for general topics. Running this requires a Python environment or a Node alternative (Node lacks mature NLP libraries, though one could use TensorFlow\.js with a smaller model like USE ). Most likely, we’d run a script in Python during build or deployment to embed all projects.
* Store the resulting vectors either in a small **vector database** like **Qdrant** or **Weaviate** (self-hosted). Both Qdrant and Weaviate are open-source and can run with minimal resources. Qdrant, for instance, is easy to run via Docker and is efficient in Rust. It even offers a free 1GB cloud which can handle our data easily. These DBs provide APIs to query by similarity and filter by metadata conditions. Alternatively, given the tiny data size, we could even store vectors in a JSON file or lightweight key-value store and perform brute-force search (the performance impact is negligible with 73 items – computing cosine similarity against each in memory is on the order of milliseconds).
* For query embedding, since we used an open-source model for indexing, we’d also need to use it at query time. This means hosting the model in a way that a query can be fed in and embedded. This could be done by running a small Python server (e.g. a Flask app with the model loaded), or by leveraging a serverless function with the model (though cold starts would be an issue), or by using something like **Hugging Face Inference API** to get embeddings from the same model (HF offers a paid-by-usage API for any hosted model, which might be an option but likely not cheaper than OpenAI in our case). There are also **ONNX-optimized models** which can run in Node.js or browser – if we wanted to embed on the client side. However, running even a MiniLM on the client might be heavy and not ideal for user experience.

**Evaluation**: This approach avoids third-party API costs entirely (aside from maybe initial model download) and gives full control. It can be extremely **cost-effective** in the long run (just the fixed cost of a small VM or leveraging free tiers). The **accuracy** can be high if a strong model is used – some open models now rival proprietary ones on benchmarks. But it requires more engineering: we must manage embedding generation infrastructure. **Ease of use** is lower because we must handle model serving and vector storage ourselves. Compatibility with Gatsby is manageable (likely involves running a separate service that Gatsby calls). If our priority is minimizing recurring costs and avoiding external dependencies, this is attractive, but given our low query volume and budget, the convenience of OpenAI or Typesense might outweigh the few dollars saved here.

To summarize the options, below is a comparison of key factors:

## Comparison of Solutions

| **Solution**                                                            | **Semantic Accuracy**                                                                                                                                                                                            | **Ease of Implementation**                                                                                                                                                                                                                                | **Cost (per \~1000 queries)**                                                                                                                                                                                                            | **Gatsby/Node Compatibility**                                                                                                                                                                                                                                      |
| ----------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| **OpenAI Embeddings API<br>+ Pinecone (Vector DB)**                     | **Very High** – Uses OpenAI’s state-of-the-art embedding model, excellent semantic relevance. Pinecone retrieval is exact (no loss in accuracy).                                                                 | **Easy** – Minimal ML code. Just call OpenAI API for vectors and use Pinecone’s API/JS client for search. No need to host ML model yourself.                                                                                                              | **Low** – OpenAI embeddings \~\$0.0001 per 1K tokens (negligible). Pinecone has free tier (sufficient here); Standard plan \~\$25/mo if ever needed. Overall <<\$20/1000 queries.                                                        | **Excellent** – Both OpenAI and Pinecone have Node.js libraries and REST endpoints. Easily integrated in a serverless function or backend. Pinecone allows metadata filters via API.                                                                               |
| **Cohere Embeddings API<br>+ Pinecone (or Qdrant)**                     | **High** – Cohere’s embedding model is strong (multilingual, high quality), comparable to OpenAI. Semantic relevance is high.                                                                                    | **Easy** – Similar to OpenAI: call Cohere API for embeddings, use vector DB API for search. Slight extra step to sign up and integrate Cohere SDK.                                                                                                        | **Low** – Cohere pricing is similar (≈\$0.40 per 1M tokens), so cost per query is also <\$0.001. Pinecone or Qdrant free tier covers storage. Total cost in range of \~\$1 or less per 1000 queries.                                     | **Excellent** – Cohere provides a Node SDK; vector DB integration same as above. No compatibility issues.                                                                                                                                                          |
| **OpenAI Embeddings API<br>+ *No separate DB* (in-memory or PGVector)** | **Very High** – Same OpenAI model as above for accuracy. (All semantic understanding from OpenAI).                                                                                                               | **Moderate** – Must write custom code to store vectors (e.g., in a JSON or in Postgres with PGVector) and compute similarity. For 73 items this is trivial (compute cosine for each on the fly, \~1 ms each). Slight coding needed but straightforward.   | **Very Low** – OpenAI cost only (few cents). No DB cost (in-memory) or negligible (using an existing Postgres). Easily under \$1 per 1000 queries.                                                                                       | **Good** – All logic can live in a Node.js serverless function (call OpenAI, then do math). No new service to integrate. Just ensure not to expose embeddings publicly.                                                                                            |
| **Typesense (Semantic Search)**                                         | **High** – Using built-in MiniLM embeddings gives good results, though slightly below OpenAI’s latest. Can integrate OpenAI for highest accuracy. Supports hybrid text+semantic matching.                        | **Moderate** – Need to run Typesense server or use cloud. Must load data and maintain index. However, setup is fairly simple (Typesense is schema-based and has JSON import APIs; plus a JS client). No ML coding needed as Typesense handles embeddings. | **Low** – Software is free (open-source). If self-hosted, \~\$5–10/mo server is sufficient. Typesense Cloud, if used, might cost more (check plans). Still within budget if self-hosted.                                                 | **Very Good** – Typesense has a Node/JS client and a REST API. Gatsby can query it directly or via a proxy. All JavaScript environment, no problem.                                                                                                                |
| **Vespa (self-host or Vespa Cloud)**                                    | **High** – Depends on embeddings used (requires external model). Can achieve high accuracy if using OpenAI or similar for vectors. Also allows advanced re-ranking.                                              | **Hard** – Steep learning curve. Need to define schema, deploy Vespa (cluster or cloud). Overkill for small data. DevOps heavy (would need to manage a Java service or use Vespa Cloud).                                                                  | **High** – Vespa Cloud geared to large deployments (e.g. \$3.36/hour for millions of vectors). For tiny scale, possibly dev tier, but likely >\$20/mo. Self-host requires powerful node (Java, 8GB+ RAM) – not cost-effective for us.    | **Okay** – Vespa has REST query API and a JavaScript client (via HTTP calls). Integration is doable, but the heavy infrastructure makes it less compatible with a simple Gatsby deployment.                                                                        |
| **Open-Source Model + Qdrant (self-host)**                              | **Medium-High** – A good open model (Sentence-BERT or similar) can perform well (within \~5-10% of OpenAI on benchmarks). Might miss subtle nuances but should handle domain-specific terms if model is general. | **Moderate** – Need to run a Python (or HF Inference) process to embed data. Qdrant setup is lightweight (Docker run). Also need a way to embed queries at runtime (maybe a small API with the model loaded). More moving parts to maintain.              | **Low** – Qdrant is free (and managed free tier available). Hosting the model could be done on same server; if using CPU, it’s slower but for low QPS that’s fine. No API costs. Possibly within a few dollars for cloud VM electricity. | **Fair** – Qdrant has REST and even a JavaScript client. Querying the DB from Gatsby is fine. The challenge is running the embedding model in Node (not ideal) or needing a separate service (Python) that Gatsby calls. This adds complexity in a JAMstack setup. |

**Table: Comparison of semantic search implementation options, considering accuracy, ease, cost, and compatibility.**

*(All options above support the core requirements of semantic search. The primary differences lie in who generates the embeddings (cloud API vs. local model) and where the vectors are stored/searched (managed service vs. self-hosted vs. in-memory).)*

## Recommended Solution and Architecture

**Recommendation:** Based on the analysis, the **most accurate and cost-effective** solution is to use **OpenAI’s embedding API** for semantic understanding, combined with a lightweight vector search implementation. Specifically, we recommend:

* **Embedding Model:** *OpenAI’s text-embedding model* (e.g. `text-embedding-ada-002` or the newer `text-embedding-3-small`). This provides excellent accuracy in capturing query-project relevance. The cost is minuscule (on the order of \$0.00002 per 1K tokens with the latest model), so even a thousand user queries barely amounts to cents. It’s effectively paying for top-tier NLP on demand, without needing to host anything. Cohere’s model is an alternative if desired, but OpenAI’s ubiquity and slightly higher benchmark performance make it a first choice.
* **Vector Store:** *In-memory or simple storage of vectors*, given the tiny dataset, to avoid extra infrastructure. We can pre-compute and store all project embeddings (perhaps in a JSON file or an embedded database). At query time, compute the query embedding via OpenAI, then perform a cosine similarity calculation against all stored vectors to find the top matches. This can be done inside an AWS Lambda function, Netlify Function, or a minimal Node backend. The computation is trivial: 73 dot-products and some math – a few milliseconds of CPU. This avoids any external DB service latency or cost, and is straightforward to implement. If down the line the portfolio grows to hundreds of items or query volume skyrockets, we could easily swap in a vector DB like Pinecone or Typesense to handle more scale. For now, simplicity wins.
* **Metadata Filtering:** Even without a dedicated search engine, we can support category-based search. Two approaches: (1) **Implicitly**, by including metadata text in the embeddings – e.g. append each project’s type/industry keywords to its description before embedding. This way, a query mentioning “therapists” will naturally match a project whose text or metadata included “therapist” or “healthcare”. (2) **Explicitly**, by parsing the query for known category keywords and applying a filter. For example, maintain a list of industries and check if the query contains one; if yes, pre-filter the candidate projects to those industry = X before computing similarity. This can be done in code. Additionally, if the UI provides filter checkboxes or dropdowns (like “Industry: Healthcare”), those selections can be sent along with the query and used to filter the vector list. Because we’ll store metadata for each project (e.g. in a JSON: `{ id, title, embedding, type, industry, ... }`), filtering that list by a field is straightforward. Combining filters with semantic ranking can be handled by first narrowing the list by filters then sorting by similarity score. Pinecone or Typesense would handle this natively too, but as noted, it’s easy enough in a custom approach due to the small scale.
* **Dynamic Results Page:** The Gatsby front-end will send the user’s query to a backend (likely an API route or serverless function). This backend will call OpenAI for the embedding and then perform the vector similarity search (plus any filtering logic). It will return a list of top N results (project IDs or data), which the front-end then displays as a results page (with project titles, thumbnails, etc., fetched from the portfolio data). The results can be sorted by descending cosine similarity score. We can also set a similarity threshold to ignore extremely low-score results (meaning the query is very unrelated to some projects).
* **Metadata Extraction Pipeline:** To enrich search and possibly display filters, we should extract structured metadata from each project. Likely the portfolio items already have some structure (tags or categories), but assuming they are just unstructured case studies, we can do a one-time **metadata extraction**. Using an LLM is an efficient way to do this: for each project description, we can prompt GPT-3.5 or similar: *“Identify the project type (UI/UX/Illustration/etc.), the industry/domain, and the main problem solved or goal of the project from the following text…”* and parse the answer. This could be done in a script. (OpenAI API cost for this would also be low given 73 items, possibly a few cents each). Another approach is to manually annotate or use keyword matching if the data is clear. Once extracted, we’ll have fields like `project_type`, `industry`, `problem_keywords` for each item. These should be stored in the search index (or the JSON with embeddings). We can use them to boost relevance (e.g. if a query mentions a specific industry, perhaps weight that project higher) or for filter UI. **Note:** The LLM extraction approach is supported by tools like Haystack’s `LLMMetadataExtractor`, which uses a prompt to get metadata from documents. This confirms the feasibility of leveraging an LLM for our metadata parsing needs.
* **Architecture Diagram:** The flow below illustrates the architecture. We have a static content source (MDX/JSON files) which during build or a preprocessing step are embedded via the AI model, and stored. At runtime, the application (front-end) sends a query to a function which embeds the query and performs similarity search to return results:

&#x20;*Architecture overview – Data (projects) are embedded by an AI model and stored (locally or in a vector DB). At query time, the application sends the user query to be embedded by the same model, then finds similar embeddings in the index (Pinecone or in-memory). The top results are returned to the front-end for display.*

As shown above, the key components are:

1. **Data Preparation**: A script (could be part of the Gatsby build or a separate pipeline) will go through each portfolio item:

   * Read the MDX or JSON content.
   * (Optionally) Use an LLM or heuristic to extract structured metadata (type, industry, problem) if not already given.
   * Send the text (and maybe title + metadata appended) to OpenAI’s Embedding API. Receive the embedding vector.
   * Store the embedding and metadata. For example, generate a JSON file `search_index.json` with an array of documents: `{ "id": <slug>, "title": ..., "embedding": [0.1, -0.7, 0.8, ...], "type": "UI Design", "industry": "Healthcare", ... }`. This file can be placed in the published site or in a cloud storage accessible by the function. (If using Pinecone or Qdrant, this step would instead upsert records into that service, including metadata fields).
2. **Query Handling (Serverless)**: Deploy a serverless function (Node.js) at an endpoint like `/api/search`. When a user enters a query on the site:

   * The front-end calls `/api/search?query=<user text>&filters=<optional filters>`.
   * The function receives the query. It loads the precomputed embeddings for projects (if small, it can keep it in memory across invocations, or quickly read the JSON – 73 items is tiny). If using a vector DB, it would instead connect to that.
   * It calls OpenAI’s embedding API to embed the query text. (This is \~300ms roundtrip typically).
   * If filters were provided (e.g. user selected “Industry: Healthcare”), the function will filter the candidate documents to those matching the filter (e.g. `doc.industry == "Healthcare"`).
   * It then computes cosine similarity between the query vector and each candidate project’s vector. This yields a score for each project. It sorts projects by score descending.
   * The top *k* (say 5–10) projects are selected. The function responds with these results (possibly including title, summary, etc., so the front-end can show a snippet).
3. **Front-End Display**: Gatsby can have a Search Results page that receives the results JSON and renders a list of project cards. Each card can link to the full case study. We can highlight which metadata matched (for instance, if we explicitly detected the query’s facets, the UI might say “Filtered by Industry: Healthcare”).
4. **Optional Enhancements**: We might implement **query understanding** – e.g., if the query is in question form or very verbose, we could use a model to rephrase or extract key terms. But given the semantic power of embeddings, this is usually unnecessary. Another enhancement is **re-ranking**: using a second-stage model (like Cohere Rerank or GPT-3.5) to refine the order of top results. With only 73 items, however, a single-stage embedding search should be sufficient. If results need fine-tuning, we could manually adjust by adding domain-specific synonyms or boosting certain fields (for example, if a query mentions an industry explicitly, ensure those projects get a slight score boost).

The proposed architecture minimizes moving parts while maximizing semantic accuracy. **Most of the heavy lifting is offloaded to an API (OpenAI)**, which is both accurate and inexpensive at our scale. By not introducing extra servers (if we avoid a persistent vector DB), we reduce maintenance and cost. Everything runs on-demand.

For a visual summary: the diagram above (from Pinecone’s documentation) conceptually matches our solution, with the “AI Model (Embedding API)” being OpenAI and “Pinecone” being either an actual Pinecone index or our in-memory index. The **data warehouse** in our case is the set of MDX/JSON files; we embed that offline. The **application** (Gatsby site) sends queries to be embedded and searched, and gets back ranked results.

## Required Components and Libraries

To implement this, the stack will include:

* **OpenAI API** – for embedding generation. Sign up for API keys and use the official OpenAI Node.js SDK (`openai` npm package) or a simple `fetch` call to their REST endpoint. Ensure to handle API key securely (store in environment variable, not expose to client).
* **Serverless Function (Node.js)** – If using Netlify, Gatsby Functions, or Vercel, create an API route that handles the search logic. Use a Node library for cosine similarity if needed (or implement a small function). Example: `cosineSimilarity(a, b) = (a·b) / (||a||*||b||)`. We can use a library like `compute-cosine-similarity` or just write our own since it’s a one-liner with vector dot products (Node can handle this easily). If filters are needed, simple JS array filter on the loaded JSON.
* **Data Storage for Index**: If keeping it simple, the `search_index.json` file can be shipped with the site (maybe not public, if we prefer, it could be bundled in the function or fetched from a private location). Alternatively, use an **embedded database** like SQLite or LowDB to store the vectors and metadata – but JSON in memory is fine here. If we opt for Pinecone or Qdrant, then their respective client libraries will be needed (e.g. `@pinecone-database/pinecone` for Node or the Qdrant REST client). For Pinecone, you’d also need the environment (API key, index name, etc.).
* **Typesense (optional)**: If we choose the Typesense route instead, we’d deploy a Typesense server (Docker or cloud). We’d use the `typesense-js` client library on the front-end or backend. Typesense would be configured with an embedding field (likely using OpenAI via API calls as described in docs, or its default model). We would then push documents to Typesense using its API (this could be done at build time via a script using their Admin API key). The front-end could query Typesense directly with a search-only API key, including `query_by=embedding` for semantic search. We’d still need to embed the query on the client or via the Typesense server making an API call – Typesense can do *auto-embedding* if configured with OpenAI keys, meaning it will call OpenAI for the query and for documents automatically. That simplifies the front-end, but we then rely on the Typesense server to reach out to OpenAI. This is a viable alternative architecture: Gatsby -> Typesense (which itself calls OpenAI as needed). The components in that case are: a running Typesense instance (with perhaps `ts/all-MiniLM-L12-v2` model bundled or OpenAI integration), and the Typesense JavaScript client.
* **Environment & Deployment**: The whole system can be deployed on a static hosting provider (for the Gatsby site) plus functions. For example, on **Netlify** we can have serverless functions that use the Node runtime. We’d include the OpenAI API key as a secure environment variable. The search index JSON could either be packaged (Netlify allows bundling files in the function) or stored in an S3 bucket or even fetched from the published site if not sensitive. Alternatively, if using **Vercel**, the Next.js functions or a standalone Node microservice could do similarly. No persistent server needed since everything is either at build time or on-demand.
* **Testing and Tuning**: We should test a variety of queries to ensure the results are relevant. We can adjust by enriching the text fed into embeddings (e.g., include not just the raw description but also any tags or titles, which often improves retrieval). We will also verify that the metadata extraction is accurate for use in filters or highlighting.

In conclusion, this architecture provides a **fast, accurate, and affordable** search solution for the Gatsby portfolio. By leveraging semantic embeddings, prospective buyers can query naturally and find the most fitting work samples even if their wording doesn’t exactly match the project text. The use of OpenAI’s embeddings ensures high relevancy, while the minimal infrastructure (just a function and a small index) keeps costs well within the \$20/1000-users range. As the portfolio grows, this setup can scale (we can introduce a vector database and caching later if needed), but for now it meets all requirements with elegant simplicity.

**Sources:**

* Meilisearch Blog – *What is semantic search?* (on proprietary vs open-source model performance, OpenAI/Cohere embeddings)
* Typesense Documentation – *Semantic Search* (on built-in embedding support and using OpenAI/PaLM)
* Pinecone Docs – *Semantic search with Pinecone* (illustrating combining LLM embeddings with a vector DB)
* Victor Dibia – *Gatsby Similar Posts with ML* (outlining approach of exporting content and computing embeddings for similarity)
* OpenAI Announcement (2024) – pricing of new embedding models (Ada-002 and text-embedding-3 cost)
* Qdrant Tech – Pricing (free 1GB vector DB tier for small projects)
* Haystack LLM Extractor – using LLM to get metadata from documents
